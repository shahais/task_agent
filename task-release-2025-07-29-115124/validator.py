#!/usr/bin/env python3
"""
SWE-bench Data Point Validator —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú SWE-bench API
"""

import json
import sys
import argparse
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Any
import logging

# –ü–†–ê–í–ò–õ–¨–ù–´–ô SWE-bench API
from swebench.harness.run_evaluation import main as run_evaluation_main
from swebench.harness.utils import load_swebench_dataset

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SWEBenchValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º SWE-bench evaluation API."""
    
    def __init__(self, timeout: int = 1800):
        self.required_fields = [
            'instance_id', 'repo', 'base_commit', 'patch', 
            'test_patch', 'problem_statement', 'hints_text', 
            'created_at', 'version', 'FAIL_TO_PASS', 'PASS_TO_PASS'
        ]
        self.timeout = timeout
    
    def validate_json_structure(self, data: Dict[str, Any]) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π."""
        errors = []
        
        for field in self.required_fields:
            if field not in data:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        if 'instance_id' in data and not isinstance(data['instance_id'], str):
            errors.append("instance_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
            
        if 'repo' in data and not isinstance(data['repo'], str):
            errors.append("repo –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç instance_id
        if 'instance_id' in data:
            instance_id = data['instance_id']
            if not instance_id or '__' not in instance_id:
                errors.append("instance_id –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '__' (—Ñ–æ—Ä–º–∞—Ç: repo__issue-number)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
        for test_field in ['FAIL_TO_PASS', 'PASS_TO_PASS']:
            if test_field in data:
                if isinstance(data[test_field], str):
                    try:
                        test_list = json.loads(data[test_field])
                        if not isinstance(test_list, list):
                            errors.append(f"{test_field} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å JSON —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫")
                    except json.JSONDecodeError:
                        errors.append(f"{test_field} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
                elif not isinstance(data[test_field], list):
                    errors.append(f"{test_field} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
            
        return errors

    def run_swebench_evaluation(self, data_point_path: str) -> Dict[str, Any]:
        """
        –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ swebench.harness.run_evaluation.main
        """
        result = {
            'evaluation_success': False,
            'patch_applied': False,
            'tests_passed': False,
            'fail_to_pass_results': {},
            'pass_to_pass_results': {},
            'errors': [],
            'logs': []
        }
        
        try:
            with open(data_point_path, 'r') as f:
                data = json.load(f)
            
            instance_id = data['instance_id']
            result['logs'].append(f"–ù–∞—á–∏–Ω–∞–µ–º SWE-bench evaluation –¥–ª—è {instance_id}")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ SWE-bench
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º golden patch –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            prediction = {
                'instance_id': instance_id,
                'model_patch': data['patch'],  # Golden patch
                'model_name_or_path': 'golden_patch_validator'
            }
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è SWE-bench
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_dir = Path(temp_dir)
                
                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç JSONL —Ñ–∞–π–ª
                dataset_file = temp_dir / 'dataset.jsonl'
                with open(dataset_file, 'w') as f:
                    json.dump(data, f)
                    f.write('\n')
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç—ã JSONL —Ñ–∞–π–ª
                predictions_file = temp_dir / 'predictions.jsonl'
                with open(predictions_file, 'w') as f:
                    json.dump(prediction, f)
                    f.write('\n')
                
                # –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
                report_dir = temp_dir / 'reports'
                report_dir.mkdir()
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π run_id
                run_id = f"validator_{uuid.uuid4().hex[:8]}"
                
                result['logs'].append("–ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π SWE-bench evaluation...")
                
                try:
                    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –≤—ã–∑–æ–≤ SWE-bench evaluation
                    report_path = run_evaluation_main(
                        dataset_name=str(dataset_file),  # –ü—É—Ç—å –∫ –Ω–∞—à–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
                        split='test',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        instance_ids=[instance_id],  # –¢–æ–ª—å–∫–æ –Ω–∞—à instance
                        predictions_path=str(predictions_file),  # –ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º
                        max_workers=1,  # –û–¥–∏–Ω –≤–æ—Ä–∫–µ—Ä –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                        force_rebuild=False,
                        cache_level='env',
                        clean=False,
                        open_file_limit=4096,
                        run_id=run_id,
                        timeout=self.timeout,
                        namespace='swebench',
                        rewrite_reports=False,
                        modal=False,
                        instance_image_tag='latest',
                        report_dir=str(report_dir)
                    )
                    
                    result['logs'].append(f"‚úì SWE-bench evaluation –∑–∞–≤–µ—Ä—à–µ–Ω, –æ—Ç—á–µ—Ç: {report_path}")
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ —á–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –æ—Ç—á–µ—Ç–∞
                    if report_path and Path(report_path).exists():
                        with open(report_path, 'r') as f:
                            report_data = json.load(f)
                        
                        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ SWE-bench evaluation
                        self._parse_swebench_report(report_data, instance_id, data, result)
                    else:
                        # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ report_dir
                        result_files = list(report_dir.glob('**/*.json'))
                        result['logs'].append(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(result_files)}")
                        
                        for result_file in result_files:
                            try:
                                with open(result_file, 'r') as f:
                                    file_data = json.load(f)
                                
                                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                                self._parse_swebench_report(file_data, instance_id, data, result)
                                break  # –í—ã—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                            except Exception as e:
                                result['logs'].append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {result_file}: {e}")
                    
                    result['evaluation_success'] = len(result['errors']) == 0
                    
                except Exception as e:
                    result['errors'].append(f"–û—à–∏–±–∫–∞ SWE-bench evaluation: {e}")
                    logger.exception("SWE-bench evaluation error")
                
        except Exception as e:
            result['errors'].append(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ evaluation: {e}")
            logger.exception("Evaluation preparation error")
        
        return result
    
    def _parse_swebench_report(self, report_data: Dict, instance_id: str, data: Dict, result: Dict):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç—á–µ—Ç–∞ SWE-bench."""
        try:
            # –§–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞ SWE-bench:
            # {
            #   "resolved_ids": ["id1", "id2"],
            #   "error_ids": ["id3"],
            #   "completed_ids": ["id1", "id2", "id3"],
            #   ...
            # }
            
            resolved_ids = report_data.get('resolved_ids', [])
            error_ids = report_data.get('error_ids', [])
            completed_ids = report_data.get('completed_ids', [])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –Ω–∞—à–µ–≥–æ instance
            if instance_id in resolved_ids:
                result['patch_applied'] = True
                result['tests_passed'] = True
                result['logs'].append(f"‚úì Instance {instance_id} —É—Å–ø–µ—à–Ω–æ resolved")
            elif instance_id in error_ids:
                result['patch_applied'] = False
                result['tests_passed'] = False
                result['errors'].append(f"Instance {instance_id} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
            elif instance_id in completed_ids:
                result['patch_applied'] = True
                result['tests_passed'] = False
                result['logs'].append(f"Instance {instance_id} completed, –Ω–æ –Ω–µ resolved")
            else:
                result['errors'].append(f"Instance {instance_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_instances = report_data.get('total_instances', 0)
            resolved_instances = report_data.get('resolved_instances', 0)
            result['logs'].append(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {resolved_instances}/{total_instances} instances resolved")
                
        except Exception as e:
            result['errors'].append(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç—á–µ—Ç–∞: {e}")
            logger.exception("Report parsing error")
    
    def validate_data_point(self, file_path: str, run_evaluation: bool = True) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Ç–æ—á–∫—É –¥–∞–Ω–Ω—ã—Ö SWE-bench."""
        result = {
            'file': file_path,
            'valid': True,
            'errors': [],
            'warnings': [],
            'structure_valid': True,
            'swe_bench_evaluation': None
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
            structure_errors = self.validate_json_structure(data)
            result['errors'].extend(structure_errors)
            result['structure_valid'] = len(structure_errors) == 0
            
            # 2. SWE-bench evaluation (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
            if run_evaluation and result['structure_valid']:
                logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º SWE-bench evaluation –¥–ª—è {file_path}")
                evaluation_result = self.run_swebench_evaluation(file_path)
                result['swe_bench_evaluation'] = evaluation_result
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ evaluation
                if not evaluation_result['evaluation_success']:
                    result['errors'].extend(evaluation_result['errors'])
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
            result['valid'] = len(result['errors']) == 0
            
        except json.JSONDecodeError as e:
            result['errors'].append(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {e}")
            result['valid'] = False
            result['structure_valid'] = False
        except Exception as e:
            result['errors'].append(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            result['valid'] = False
        
        return result
    
    def validate_batch(self, file_paths: List[str], run_evaluation: bool = True) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç —Ñ–∞–π–ª–æ–≤."""
        results = []
        
        for file_path in file_paths:
            logger.info(f"–í–∞–ª–∏–¥–∏—Ä—É–µ–º {file_path}")
            result = self.validate_data_point(file_path, run_evaluation)
            results.append(result)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        valid_count = sum(1 for r in results if r['valid'])
        total_count = len(results)
        
        return {
            'results': results,
            'summary': {
                'total': total_count,
                'valid': valid_count,
                'invalid': total_count - valid_count,
                'success_rate': valid_count / total_count if total_count > 0 else 0
            }
        }


def main():
    parser = argparse.ArgumentParser(description='SWE-bench Data Point Validator')
    parser.add_argument('files', nargs='+', help='JSON —Ñ–∞–π–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
    parser.add_argument('--no-evaluation', action='store_true', 
                       help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å SWE-bench evaluation')
    parser.add_argument('--timeout', type=int, default=1800,
                       help='Timeout –¥–ª—è evaluation (—Å–µ–∫—É–Ω–¥—ã)')
    parser.add_argument('--verbose', action='store_true',
                       help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    validator = SWEBenchValidator(timeout=args.timeout)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    batch_result = validator.validate_batch(args.files, not args.no_evaluation)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for result in batch_result['results']:
        status = "‚úì VALID" if result['valid'] else "‚úó INVALID"
        print(f"{status}: {result['file']}")
        
        if result['errors']:
            for error in result['errors']:
                print(f"  ERROR: {error}")
        
        if result['swe_bench_evaluation']:
            eval_result = result['swe_bench_evaluation']
            print(f"  SWE-bench evaluation:")
            print(f"    Patch applied: {'‚úì' if eval_result['patch_applied'] else '‚úó'}")
            print(f"    Tests passed: {'‚úì' if eval_result['tests_passed'] else '‚úó'}")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    summary = batch_result['summary']
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {summary['valid']}/{summary['total']} —Ñ–∞–π–ª–æ–≤ –≤–∞–ª–∏–¥–Ω—ã")
    print("üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –û–§–ò–¶–ò–ê–õ–¨–ù–´–ô SWE-bench evaluation harness —Å Docker")
    
    # –ö–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞
    sys.exit(0 if summary['valid'] == summary['total'] else 1)


if __name__ == '__main__':
    main()
