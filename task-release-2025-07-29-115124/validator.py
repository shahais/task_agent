#!/usr/bin/env python3
"""
SWE-bench Data Point Validator —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú SWE-bench API
"""

import json
import sys
import argparse
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Any
import logging

# –ü–†–ê–í–ò–õ–¨–ù–´–ô SWE-bench API
from swebench.harness.run_evaluation import main as run_evaluation_main
from swebench.harness.utils import load_swebench_dataset

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SWEBenchValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º SWE-bench evaluation API."""
    
    def __init__(self, timeout: int = 1800):
        self.required_fields = [
            'instance_id', 'repo', 'base_commit', 'patch', 
            'test_patch', 'problem_statement', 'hints_text', 
            'created_at', 'version', 'FAIL_TO_PASS', 'PASS_TO_PASS'
        ]
        self.timeout = timeout
    
    def validate_json_structure(self, data: Dict[str, Any]) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π."""
        errors = []
        
        for field in self.required_fields:
            if field not in data:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        if 'instance_id' in data and not isinstance(data['instance_id'], str):
            errors.append("instance_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
            
        if 'repo' in data and not isinstance(data['repo'], str):
            errors.append("repo –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç instance_id
        if 'instance_id' in data:
            instance_id = data['instance_id']
            if not instance_id or '__' not in instance_id:
                errors.append("instance_id –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å '__' (—Ñ–æ—Ä–º–∞—Ç: repo__issue-number)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
        for test_field in ['FAIL_TO_PASS', 'PASS_TO_PASS']:
            if test_field in data:
                if isinstance(data[test_field], str):
                    try:
                        test_list = json.loads(data[test_field])
                        if not isinstance(test_list, list):
                            errors.append(f"{test_field} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å JSON —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫")
                    except json.JSONDecodeError:
                        errors.append(f"{test_field} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
                elif not isinstance(data[test_field], list):
                    errors.append(f"{test_field} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
            
        return errors

    def run_swebench_evaluation(self, data_point_path: str) -> Dict[str, Any]:
        """
        –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ swebench.harness.run_evaluation.main
        """
        result = {
            'evaluation_success': False,
            'patch_applied': False,
            'tests_passed': False,
            'fail_to_pass_results': {},
            'pass_to_pass_results': {},
            'errors': [],
            'logs': []
        }
        
        try:
            with open(data_point_path, 'r') as f:
                data = json.load(f)
            
            instance_id = data['instance_id']
            result['logs'].append(f"–ù–∞—á–∏–Ω–∞–µ–º SWE-bench evaluation –¥–ª—è {instance_id}")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ SWE-bench
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º golden patch –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            prediction = {
                'instance_id': instance_id,
                'model_patch': data['patch'],  # Golden patch
                'model_name_or_path': 'golden_patch_validator'
            }
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è SWE-bench
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_dir = Path(temp_dir)
                
                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç JSONL —Ñ–∞–π–ª
                dataset_file = temp_dir / 'dataset.jsonl'
                with open(dataset_file, 'w') as f:
                    json.dump(data, f)
                    f.write('\n')
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç—ã JSONL —Ñ–∞–π–ª
                predictions_file = temp_dir / 'predictions.jsonl'
                with open(predictions_file, 'w') as f:
                    json.dump(prediction, f)
                    f.write('\n')
                
                # –ü–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
                report_dir = temp_dir / 'reports'
                report_dir.mkdir()
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π run_id
                run_id = f"validator_{uuid.uuid4().hex[:8]}"
                
                result['logs'].append("–ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π SWE-bench evaluation...")
                
                try:
                    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –≤—ã–∑–æ–≤ SWE-bench evaluation
                    report_path = run_evaluation_main(
                        dataset_name=str(dataset_file),  # –ü—É—Ç—å –∫ –Ω–∞—à–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
                        split='test',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        instance_ids=[instance_id],  # –¢–æ–ª—å–∫–æ –Ω–∞—à instance
                        predictions_path=str(predictions_file),  # –ü—É—Ç—å –∫ –ø—Ä–µ–¥–∏–∫—Ç–∞–º
                        max_workers=1,  # –û–¥–∏–Ω –≤–æ—Ä–∫–µ—Ä –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                        force_rebuild=False,
                        cache_level='env',
                        clean=False,
                        open_file_limit=4096,
                        run_id=run_id,
                        timeout=self.timeout,
                        namespace='swebench',
                        rewrite_reports=False,
                        modal=False,
                        instance_image_tag='latest',
                        report_dir=str(report_dir)
                    )
                    
                    result['logs'].append(f"‚úì SWE-bench evaluation –∑–∞–≤–µ—Ä—à–µ–Ω, –æ—Ç—á–µ—Ç: {report_path}")
                    
                    # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –æ—Ç—á–µ—Ç–∞
                    if report_path and Path(report_path).exists():
                        with open(report_path, 'r') as f:
                            report_data = json.load(f)
                        
                        # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –Ω–∞—à–µ–≥–æ instance
                        for entry in report_data:
                            if entry.get('instance_id') == instance_id:
                                self._parse_evaluation_result(entry, data, result)
                                break
                    else:
                        # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ report_dir
                        result_files = list(report_dir.glob('**/*.json'))
                        result['logs'].append(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(result_files)}")
                        
                        for result_file in result_files:
                            try:
                                with open(result_file, 'r') as f:
                                    file_data = json.load(f)
                                
                                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞—à instance –≤ —Ñ–∞–π–ª–µ
                                if isinstance(file_data, list):
                                    for entry in file_data:
                                        if entry.get('instance_id') == instance_id:
                                            self._parse_evaluation_result(entry, data, result)
                                            break
                                elif isinstance(file_data, dict) and file_data.get('instance_id') == instance_id:
                                    self._parse_evaluation_result(file_data, data, result)
                                    break
                            except Exception as e:
                                result['logs'].append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {result_file}: {e}")
                    
                    result['evaluation_success'] = len(result['errors']) == 0
                    
                except Exception as e:
                    result['errors'].append(f"–û—à–∏–±–∫–∞ SWE-bench evaluation: {e}")
                    logger.exception("SWE-bench evaluation error")
                
        except Exception as e:
            result['errors'].append(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ evaluation: {e}")
            logger.exception("Evaluation preparation error")
        
        return result
    
    def _parse_evaluation_result(self, eval_entry: Dict, data: Dict, result: Dict):
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç evaluation."""
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        result['patch_applied'] = eval_entry.get('patch_applied', False)
        result['tests_passed'] = eval_entry.get('resolved', False)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
        test_results = eval_entry.get('test_results', {})
        
        # FAIL_TO_PASS —Ç–µ—Å—Ç—ã
        fail_to_pass = json.loads(data.get('FAIL_TO_PASS', '[]'))
        for test in fail_to_pass:
            # –í SWE-bench —Å—Ç–∞—Ç—É—Å –º–æ–∂–µ—Ç –±—ã—Ç—å PASSED, FAILED, ERROR, TIMEOUT
            test_status = test_results.get(test, {})
            if isinstance(test_status, dict):
                test_passed = test_status.get('status') == 'PASSED'
            else:
                # –ò–Ω–æ–≥–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–æ–π
                test_passed = test_status == 'PASSED'
            
            result['fail_to_pass_results'][test] = {'passed': test_passed}
            if not test_passed:
                result['errors'].append(f"FAIL_TO_PASS —Ç–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª: {test}")
        
        # PASS_TO_PASS —Ç–µ—Å—Ç—ã (–±–µ—Ä–µ–º sample –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        pass_to_pass = json.loads(data.get('PASS_TO_PASS', '[]'))
        sample_tests = pass_to_pass[:5]  # –ü–µ—Ä–≤—ã–µ 5 —Ç–µ—Å—Ç–æ–≤
        for test in sample_tests:
            test_status = test_results.get(test, {})
            if isinstance(test_status, dict):
                test_passed = test_status.get('status') == 'PASSED'
            else:
                test_passed = test_status == 'PASSED'
            
            result['pass_to_pass_results'][test] = {'passed': test_passed}
            if not test_passed:
                result['errors'].append(f"PASS_TO_PASS —Ç–µ—Å—Ç —Å–ª–æ–º–∞–ª—Å—è: {test}")
        
        result['logs'].append(f"Parsed results - patch_applied: {result['patch_applied']}, tests_passed: {result['tests_passed']}")
    
    def validate_data_point(self, file_path: str, run_evaluation: bool = True) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Ç–æ—á–∫—É –¥–∞–Ω–Ω—ã—Ö SWE-bench."""
        result = {
            'file': file_path,
            'valid': True,
            'errors': [],
            'warnings': [],
            'structure_valid': True,
            'swe_bench_evaluation': None
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
            structure_errors = self.validate_json_structure(data)
            result['errors'].extend(structure_errors)
            result['structure_valid'] = len(structure_errors) == 0
            
            # 2. –ü–†–ê–í–ò–õ–¨–ù–ê–Ø SWE-bench evaluation
            if run_evaluation and result['structure_valid']:
                logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º SWE-bench evaluation –¥–ª—è {file_path}")
                swe_result = self.run_swebench_evaluation(file_path)
                result['swe_bench_evaluation'] = swe_result
                
                if not swe_result['evaluation_success']:
                    result['errors'].extend(swe_result['errors'])
            
            result['valid'] = len(result['errors']) == 0
            
        except json.JSONDecodeError as e:
            result['errors'].append(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON: {e}")
            result['valid'] = False
            result['structure_valid'] = False
        except FileNotFoundError:
            result['errors'].append("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            result['valid'] = False
        except Exception as e:
            result['errors'].append(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            result['valid'] = False
            logger.exception("Validation error")
        
        return result


def main():
    parser = argparse.ArgumentParser(description='SWE-bench Data Point Validator —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º API')
    parser.add_argument('files', nargs='+', help='JSON —Ñ–∞–π–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
    parser.add_argument('--json', action='store_true', help='–í—ã–≤–æ–¥ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ')
    parser.add_argument('--verbose', '-v', action='store_true', help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    parser.add_argument('--no-evaluation', action='store_true', 
                       help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å SWE-bench evaluation (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)')
    parser.add_argument('--timeout', type=int, default=1800, 
                       help='–¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1800)')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    validator = SWEBenchValidator(timeout=args.timeout)
    results = []
    
    for file_path in args.files:
        logger.info(f"–í–∞–ª–∏–¥–∏—Ä—É–µ–º {file_path}")
        result = validator.validate_data_point(file_path, run_evaluation=not args.no_evaluation)
        results.append(result)
        
        if not args.json:
            status = "‚úì VALID" if result['valid'] else "‚úó INVALID"
            print(f"{status}: {file_path}")
            
            if result['errors']:
                for error in result['errors']:
                    print(f"  ERROR: {error}")
            
            if result['warnings'] and args.verbose:
                for warning in result['warnings']:
                    print(f"  WARNING: {warning}")
            
            # –î–µ—Ç–∞–ª–∏ SWE-bench evaluation
            if args.verbose and result.get('swe_bench_evaluation'):
                swe_result = result['swe_bench_evaluation']
                print(f"  SWE-bench evaluation:")
                print(f"    Patch applied: {'‚úì' if swe_result.get('patch_applied') else '‚úó'}")
                print(f"    Tests passed: {'‚úì' if swe_result.get('tests_passed') else '‚úó'}")
                
                fail_to_pass = swe_result.get('fail_to_pass_results', {})
                if fail_to_pass:
                    passed = sum(1 for r in fail_to_pass.values() if r['passed'])
                    total = len(fail_to_pass)
                    print(f"    FAIL_TO_PASS: {passed}/{total} passed")
                
                pass_to_pass = swe_result.get('pass_to_pass_results', {})
                if pass_to_pass:
                    passed = sum(1 for r in pass_to_pass.values() if r['passed'])
                    total = len(pass_to_pass)
                    print(f"    PASS_TO_PASS: {passed}/{total} passed (sample)")
    
    if args.json:
        print(json.dumps(results, indent=2, ensure_ascii=False))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    valid_count = sum(1 for r in results if r['valid'])
    total_count = len(results)
    
    if not args.json:
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {valid_count}/{total_count} —Ñ–∞–π–ª–æ–≤ –≤–∞–ª–∏–¥–Ω—ã")
        if not args.no_evaluation:
            print("üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –û–§–ò–¶–ò–ê–õ–¨–ù–´–ô SWE-bench evaluation harness —Å Docker")
        else:
            print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
    
    # –í—ã—Ö–æ–¥ —Å –∫–æ–¥–æ–º –æ—à–∏–±–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Ñ–∞–π–ª—ã
    if valid_count < total_count:
        sys.exit(1)


if __name__ == "__main__":
    main()
